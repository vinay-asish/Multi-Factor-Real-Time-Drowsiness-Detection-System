{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f07d93e-8691-46c2-a3cd-954bd02668fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from scipy.spatial import distance as dist\n",
    "import time\n",
    "import pygame\n",
    "import threading\n",
    "from collections import deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7754d05e-80a7-46f9-8389-b0316d8e0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MediaPipeDrowsinessDetector:\n",
    "#     def __init__(self):\n",
    "#         # Initialize MediaPipe Face Mesh\n",
    "#         self.mp_face_mesh = mp.solutions.face_mesh\n",
    "#         self.mp_drawing = mp.solutions.drawing_utils\n",
    "#         self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        \n",
    "#         # Initialize Face Mesh model\n",
    "#         self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "#             max_num_faces=1,\n",
    "#             refine_landmarks=True,\n",
    "#             min_detection_confidence=0.5,\n",
    "#             min_tracking_confidence=0.5\n",
    "#         )\n",
    "        \n",
    "#         # COMPREHENSIVE MediaPipe Face Mesh landmark indices with MAXIMUM POINTS around eyes and mouth\n",
    "#         # Left eye landmarks - ALL possible eye-related points for maximum precision\n",
    "#         self.LEFT_EYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246, \n",
    "#                         130, 25, 110, 24, 23, 22, 26, 112, 243, 190, 56, 28, 27, 29, 30, 247, 31, 226, \n",
    "#                         113, 225, 224, 223, 222, 221, 189, 244, 245, 122, 6, 202, 214, 234, 227, 116, \n",
    "#                         117, 118, 119, 120, 121, 128, 126, 142, 36, 205, 206, 207, 213, 192, 147, 123, \n",
    "#                         124, 139, 71, 68, 104, 69, 108, 151, 35, 31, 228, 229, 230, 231, 232, 233, 244, \n",
    "#                         245, 122, 6, 202, 214, 234, 227, 116, 117, 118, 119, 120, 121, 128, 126, 142, \n",
    "#                         36, 205, 206, 207, 213, 192, 147, 123, 124, 139, 71, 68, 104, 69, 108, 9, 10, \n",
    "#                         151, 337, 299, 333, 298, 301, 251, 284, 332, 297, 338, 10, 109, 67, 103, 54, 21]\n",
    "        \n",
    "#         # Right eye landmarks - ALL possible eye-related points for maximum precision\n",
    "#         self.RIGHT_EYE = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398,\n",
    "#                          359, 255, 339, 254, 253, 252, 256, 341, 463, 414, 286, 258, 257, 259, 260, 467,\n",
    "#                          261, 446, 342, 445, 444, 443, 442, 441, 413, 464, 435, 410, 454, 365, 397, 288,\n",
    "#                          361, 340, 346, 347, 348, 349, 350, 451, 452, 453, 464, 435, 410, 454, 365, 397,\n",
    "#                          288, 361, 340, 346, 347, 348, 349, 350, 451, 452, 453, 464, 435, 410, 278, 294,\n",
    "#                          295, 296, 334, 293, 300, 276, 283, 282, 295, 285, 336, 285, 336, 296, 334, 293,\n",
    "#                          300, 276, 283, 282, 448, 449, 450, 451, 452, 453, 464, 435, 410, 454, 365, 397,\n",
    "#                          288, 361, 340, 346, 347, 348, 349, 350, 451, 463, 414, 286, 258, 257, 259, 260]\n",
    "        \n",
    "#         # ENHANCED: More precise eye landmarks for EAR calculation\n",
    "#         # Left eye: outer corner, multiple top points, inner corner, multiple bottom points\n",
    "#         self.LEFT_EYE_SIMPLE = [33, 160, 159, 158, 133, 153, 145, 144]  # 8 points for better precision\n",
    "#         # Right eye: outer corner, multiple top points, inner corner, multiple bottom points  \n",
    "#         self.RIGHT_EYE_SIMPLE = [362, 385, 386, 387, 263, 373, 374, 380]  # 8 points for better precision\n",
    "        \n",
    "#         # COMPREHENSIVE: Maximum mouth landmarks for superior MAR calculation\n",
    "#         self.MOUTH_SIMPLE = [0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 24, 25, 26, 27, 28, 29, 30, 31, 32, \n",
    "#                             33, 37, 39, 40, 41, 42, 61, 62, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 84, 85, \n",
    "#                             86, 87, 88, 89, 90, 91, 95, 96, 146, 178, 179, 180, 181, 183, 184, 185, 186, 191, \n",
    "#                             267, 268, 269, 270, 271, 272, 273, 291, 292, 302, 303, 304, 305, 306, 307, 308, \n",
    "#                             310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 324, 325, 375, 402, \n",
    "#                             403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 415, 308, 324, 318, 402, 317, \n",
    "#                             14, 87, 178, 88, 95, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 324, 318, \n",
    "#                             405, 320, 307, 375, 321, 308, 324, 318, 267, 269, 270, 267, 271, 272]\n",
    "        \n",
    "#         # Head pose estimation landmarks\n",
    "#         self.FACE_OVAL = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67]\n",
    "        \n",
    "#         # ADJUSTED: More realistic thresholds\n",
    "#         self.EAR_THRESHOLD = 0.21\n",
    "#         self.MAR_THRESHOLD = 0.7   \n",
    "#         self.HEAD_POSE_THRESHOLD = 45  # FIXED: Increased threshold to reduce false positives\n",
    "#         self.CONSECUTIVE_FRAMES = 25   \n",
    "        \n",
    "#         # Counters and tracking variables\n",
    "#         self.ear_counter = 0\n",
    "#         self.mar_counter = 0\n",
    "#         self.head_pose_counter = 0\n",
    "#         self.blink_counter = 0\n",
    "#         self.total_blinks = 0\n",
    "        \n",
    "#         # Temporal tracking\n",
    "#         self.ear_history = deque(maxlen=30)\n",
    "#         self.mar_history = deque(maxlen=30)\n",
    "#         self.head_pose_history = deque(maxlen=30)\n",
    "#         self.blink_history = deque(maxlen=300)  # 10 seconds at 30fps\n",
    "        \n",
    "#         # FIXED: Head pose calibration - store initial pose as reference\n",
    "#         self.reference_pose = None\n",
    "#         self.pose_calibrated = False\n",
    "#         self.calibration_frames = 0\n",
    "#         self.calibration_poses = []\n",
    "        \n",
    "#         # Alert system with continuous beeping\n",
    "#         pygame.mixer.init()\n",
    "#         self.alert_sound = None\n",
    "#         self.load_alert_sound()\n",
    "#         self.alert_playing = False\n",
    "#         self.alert_channel = None\n",
    "        \n",
    "#         # Previous eye state for blink detection\n",
    "#         self.prev_eye_closed = False\n",
    "        \n",
    "#         # Time tracking\n",
    "#         self.start_time = time.time()\n",
    "        \n",
    "#     def load_alert_sound(self):\n",
    "#         \"\"\"Load alert sound with continuous beeping capability\"\"\"\n",
    "#         try:\n",
    "#             # Try to load a custom alert sound\n",
    "#             self.alert_sound = pygame.mixer.Sound('alert.wav')\n",
    "#         except:\n",
    "#             # Create a continuous beep sound programmatically\n",
    "#             sample_rate = 22050\n",
    "#             duration = 0.5  # Shorter beep for continuous effect\n",
    "#             frequency = 1000  # Higher frequency for urgency\n",
    "#             t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "            \n",
    "#             # Create a more urgent beep pattern\n",
    "#             wave = np.sin(frequency * 2 * np.pi * t) * 0.4\n",
    "#             # Add fade in/out to avoid clicking\n",
    "#             fade_samples = int(0.05 * sample_rate)  # 50ms fade\n",
    "#             wave[:fade_samples] *= np.linspace(0, 1, fade_samples)\n",
    "#             wave[-fade_samples:] *= np.linspace(1, 0, fade_samples)\n",
    "            \n",
    "#             sound_array = (wave * 32767).astype(np.int16)\n",
    "#             sound_array = np.repeat(sound_array.reshape(len(sound_array), 1), 2, axis=1)\n",
    "#             self.alert_sound = pygame.sndarray.make_sound(sound_array)\n",
    "    \n",
    "#     def get_landmarks(self, landmarks, indices, width, height):\n",
    "#         \"\"\"Extract specific landmarks and convert to pixel coordinates\"\"\"\n",
    "#         points = []\n",
    "#         for idx in indices:\n",
    "#             x = int(landmarks.landmark[idx].x * width)\n",
    "#             y = int(landmarks.landmark[idx].y * height)\n",
    "#             points.append([x, y])\n",
    "#         return np.array(points)\n",
    "    \n",
    "#     def eye_aspect_ratio(self, eye_landmarks):\n",
    "#         \"\"\"Enhanced Eye Aspect Ratio (EAR) calculation using multiple points\"\"\"\n",
    "#         try:\n",
    "#             # Multiple vertical distances for better accuracy\n",
    "#             A1 = dist.euclidean(eye_landmarks[1], eye_landmarks[7])  # Top to bottom (vertical 1)\n",
    "#             A2 = dist.euclidean(eye_landmarks[2], eye_landmarks[6])  # Top to bottom (vertical 2)\n",
    "#             A3 = dist.euclidean(eye_landmarks[1], eye_landmarks[6])  # Cross vertical\n",
    "#             A4 = dist.euclidean(eye_landmarks[2], eye_landmarks[7])  # Cross vertical\n",
    "            \n",
    "#             # Horizontal distance (width of the eye)\n",
    "#             C = dist.euclidean(eye_landmarks[0], eye_landmarks[4])  # Outer corner to inner corner\n",
    "            \n",
    "#             # Enhanced EAR formula using multiple measurements\n",
    "#             if C == 0:  # Prevent division by zero\n",
    "#                 return 0.3\n",
    "            \n",
    "#             ear = (A1 + A2 + A3 + A4) / (4.0 * C)\n",
    "#             return ear\n",
    "#         except:\n",
    "#             return 0.3\n",
    "    \n",
    "#     def mouth_aspect_ratio(self, mouth_landmarks):\n",
    "#         \"\"\"Enhanced Mouth Aspect Ratio (MAR) calculation using multiple points\"\"\"\n",
    "#         try:\n",
    "#             # Multiple vertical mouth measurements for better accuracy\n",
    "#             A1 = dist.euclidean(mouth_landmarks[1], mouth_landmarks[7])   # Top to bottom (center)\n",
    "#             A2 = dist.euclidean(mouth_landmarks[2], mouth_landmarks[6])   # Top to bottom (left-center)\n",
    "#             A3 = dist.euclidean(mouth_landmarks[3], mouth_landmarks[5])   # Top to bottom (right-center)\n",
    "#             A4 = dist.euclidean(mouth_landmarks[13], mouth_landmarks[19]) # Inner vertical distance\n",
    "#             A5 = dist.euclidean(mouth_landmarks[14], mouth_landmarks[18]) # Inner vertical distance\n",
    "            \n",
    "#             # Horizontal mouth distance (width)\n",
    "#             C1 = dist.euclidean(mouth_landmarks[0], mouth_landmarks[4])   # Left corner to right corner\n",
    "#             C2 = dist.euclidean(mouth_landmarks[12], mouth_landmarks[16]) # Inner horizontal distance\n",
    "            \n",
    "#             if C1 == 0 or C2 == 0:  # Prevent division by zero\n",
    "#                 return 0.3\n",
    "                \n",
    "#             # Enhanced MAR formula using multiple measurements\n",
    "#             mar = (A1 + A2 + A3 + A4 + A5) / (3.0 * (C1 + C2))\n",
    "#             return mar\n",
    "#         except:\n",
    "#             return 0.3\n",
    "    \n",
    "#     def get_head_pose_mediapipe(self, landmarks, width, height):\n",
    "#         \"\"\"Estimate head pose using MediaPipe 3D landmarks\"\"\"\n",
    "#         # Key 3D model points for head pose estimation\n",
    "#         model_points = np.array([\n",
    "#             (0.0, 0.0, 0.0),             # Nose tip\n",
    "#             (0.0, -330.0, -65.0),        # Chin\n",
    "#             (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "#             (225.0, 170.0, -135.0),      # Right eye right corner  \n",
    "#             (-150.0, -150.0, -125.0),    # Left mouth corner\n",
    "#             (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "#         ])\n",
    "        \n",
    "#         # Corresponding 2D image points from MediaPipe landmarks\n",
    "#         image_points = np.array([\n",
    "#             [landmarks.landmark[1].x * width, landmarks.landmark[1].y * height],    # Nose tip\n",
    "#             [landmarks.landmark[152].x * width, landmarks.landmark[152].y * height], # Chin\n",
    "#             [landmarks.landmark[33].x * width, landmarks.landmark[33].y * height],   # Left eye corner\n",
    "#             [landmarks.landmark[263].x * width, landmarks.landmark[263].y * height], # Right eye corner\n",
    "#             [landmarks.landmark[61].x * width, landmarks.landmark[61].y * height],   # Left mouth corner\n",
    "#             [landmarks.landmark[291].x * width, landmarks.landmark[291].y * height]  # Right mouth corner\n",
    "#         ], dtype=\"double\")\n",
    "        \n",
    "#         # Camera internals\n",
    "#         focal_length = width\n",
    "#         center = (width/2, height/2)\n",
    "#         camera_matrix = np.array([\n",
    "#             [focal_length, 0, center[0]],\n",
    "#             [0, focal_length, center[1]],\n",
    "#             [0, 0, 1]\n",
    "#         ], dtype=\"double\")\n",
    "        \n",
    "#         dist_coeffs = np.zeros((4,1))\n",
    "        \n",
    "#         # Solve PnP\n",
    "#         try:\n",
    "#             success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "#                 model_points, image_points, camera_matrix, dist_coeffs, \n",
    "#                 flags=cv2.SOLVEPNP_ITERATIVE\n",
    "#             )\n",
    "            \n",
    "#             if success:\n",
    "#                 # Convert rotation vector to angles\n",
    "#                 rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "#                 angles, _, _, _, _, _ = cv2.RQDecomp3x3(rotation_matrix)\n",
    "#                 return angles[0], angles[1], angles[2]  # pitch, yaw, roll\n",
    "#             else:\n",
    "#                 return 0, 0, 0\n",
    "#         except:\n",
    "#             return 0, 0, 0\n",
    "    \n",
    "#     def calibrate_head_pose(self, head_angles):\n",
    "#         \"\"\"FIXED: Calibrate reference head pose for better detection\"\"\"\n",
    "#         if not self.pose_calibrated:\n",
    "#             self.calibration_poses.append(head_angles)\n",
    "#             self.calibration_frames += 1\n",
    "            \n",
    "#             if self.calibration_frames >= 30:  # Calibrate over 30 frames\n",
    "#                 # Calculate average pose as reference\n",
    "#                 avg_pitch = np.mean([pose[0] for pose in self.calibration_poses])\n",
    "#                 avg_yaw = np.mean([pose[1] for pose in self.calibration_poses])\n",
    "#                 avg_roll = np.mean([pose[2] for pose in self.calibration_poses])\n",
    "                \n",
    "#                 self.reference_pose = (avg_pitch, avg_yaw, avg_roll)\n",
    "#                 self.pose_calibrated = True\n",
    "#                 print(f\"Head pose calibrated: P:{avg_pitch:.1f} Y:{avg_yaw:.1f} R:{avg_roll:.1f}\")\n",
    "                \n",
    "#         return self.pose_calibrated\n",
    "    \n",
    "#     def detect_blink(self, ear):\n",
    "#         \"\"\"Detect blinks based on EAR\"\"\"\n",
    "#         eye_closed = ear < self.EAR_THRESHOLD\n",
    "        \n",
    "#         # Detect transition from closed to open (blink completion)\n",
    "#         if self.prev_eye_closed and not eye_closed:\n",
    "#             self.total_blinks += 1\n",
    "#             self.blink_history.append(time.time())\n",
    "        \n",
    "#         self.prev_eye_closed = eye_closed\n",
    "#         return eye_closed\n",
    "    \n",
    "#     def get_blink_frequency(self):\n",
    "#         \"\"\"Calculate blinks per minute\"\"\"\n",
    "#         current_time = time.time()\n",
    "#         # Remove blinks older than 60 seconds\n",
    "#         self.blink_history = deque([t for t in self.blink_history if current_time - t <= 60], \n",
    "#                                   maxlen=300)\n",
    "#         return len(self.blink_history)\n",
    "    \n",
    "#     def analyze_drowsiness(self, ear, mar, head_angles, blink_freq):\n",
    "#         \"\"\"Multi-factor drowsiness analysis\"\"\"\n",
    "#         drowsiness_indicators = {\n",
    "#             'eye_closure': False,\n",
    "#             'yawning': False,\n",
    "#             'head_nodding': False,\n",
    "#             'low_blink_rate': False\n",
    "#         }\n",
    "        \n",
    "#         drowsiness_score = 0\n",
    "        \n",
    "#         # FIXED: Eye closure detection with better logic\n",
    "#         if ear < self.EAR_THRESHOLD:\n",
    "#             self.ear_counter += 1\n",
    "#             if self.ear_counter >= self.CONSECUTIVE_FRAMES:\n",
    "#                 drowsiness_indicators['eye_closure'] = True\n",
    "#                 drowsiness_score += 40\n",
    "#         else:\n",
    "#             self.ear_counter = 0\n",
    "        \n",
    "#         # FIXED: Yawn detection with better threshold\n",
    "#         if mar > self.MAR_THRESHOLD:\n",
    "#             self.mar_counter += 1\n",
    "#             if self.mar_counter >= 15:  # Increased threshold for yawning\n",
    "#                 drowsiness_indicators['yawning'] = True\n",
    "#                 drowsiness_score += 25\n",
    "#         else:\n",
    "#             self.mar_counter = 0\n",
    "        \n",
    "#         # FIXED: Head pose analysis with calibration\n",
    "#         if self.pose_calibrated:\n",
    "#             pitch, yaw, roll = head_angles\n",
    "#             ref_pitch, ref_yaw, ref_roll = self.reference_pose\n",
    "            \n",
    "#             # Calculate deviations from reference pose\n",
    "#             pitch_deviation = abs(pitch - ref_pitch)\n",
    "#             yaw_deviation = abs(yaw - ref_yaw)\n",
    "#             roll_deviation = abs(roll - ref_roll)\n",
    "            \n",
    "#             # Only trigger if significant deviation from normal pose\n",
    "#             if (pitch_deviation > self.HEAD_POSE_THRESHOLD or \n",
    "#                 yaw_deviation > self.HEAD_POSE_THRESHOLD or \n",
    "#                 roll_deviation > self.HEAD_POSE_THRESHOLD):\n",
    "#                 self.head_pose_counter += 1\n",
    "#                 if self.head_pose_counter >= 30:  # Increased threshold\n",
    "#                     drowsiness_indicators['head_nodding'] = True\n",
    "#                     drowsiness_score += 20\n",
    "#             else:\n",
    "#                 self.head_pose_counter = 0\n",
    "        \n",
    "#         # Low blink rate detection (normal: 15-20 blinks/minute)\n",
    "#         if blink_freq < 6:  # Very low blink rate\n",
    "#             drowsiness_indicators['low_blink_rate'] = True\n",
    "#             drowsiness_score += 15\n",
    "        \n",
    "#         # Store history for temporal analysis\n",
    "#         self.ear_history.append(ear)\n",
    "#         self.mar_history.append(mar)\n",
    "#         self.head_pose_history.append(head_angles)\n",
    "        \n",
    "#         return drowsiness_indicators, drowsiness_score\n",
    "    \n",
    "#     def trigger_alert(self, drowsiness_score, indicators):\n",
    "#         \"\"\"Trigger appropriate alerts with continuous beeping for critical states\"\"\"\n",
    "#         if drowsiness_score >= 60:  # High drowsiness\n",
    "#             alert_level = \"CRITICAL\"\n",
    "#             color = (0, 0, 255)  # Red\n",
    "#             should_beep = True\n",
    "#         elif drowsiness_score >= 35:  # Moderate drowsiness\n",
    "#             alert_level = \"WARNING\"\n",
    "#             color = (0, 165, 255)  # Orange\n",
    "#             should_beep = True\n",
    "#         elif drowsiness_score >= 20:  # Mild drowsiness\n",
    "#             alert_level = \"CAUTION\"\n",
    "#             color = (0, 255, 255)  # Yellow\n",
    "#             should_beep = False\n",
    "#         else:\n",
    "#             alert_level = \"NORMAL\"\n",
    "#             color = (0, 255, 0)  # Green\n",
    "#             should_beep = False\n",
    "        \n",
    "#         # CONTINUOUS BEEPING LOGIC\n",
    "#         if should_beep and self.alert_sound:\n",
    "#             if not self.alert_playing:\n",
    "#                 # Start continuous beeping\n",
    "#                 try:\n",
    "#                     self.alert_channel = self.alert_sound.play(-1)  # -1 means loop indefinitely\n",
    "#                     self.alert_playing = True\n",
    "#                     print(f\"ðŸš¨ ALERT STARTED: {alert_level}\")\n",
    "#                 except:\n",
    "#                     pass\n",
    "#         else:\n",
    "#             # Stop beeping if alert level is low\n",
    "#             if self.alert_playing:\n",
    "#                 try:\n",
    "#                     if self.alert_channel:\n",
    "#                         self.alert_channel.stop()\n",
    "#                     self.alert_playing = False\n",
    "#                     print(f\"âœ… ALERT STOPPED: {alert_level}\")\n",
    "#                 except:\n",
    "#                     pass\n",
    "        \n",
    "#         return alert_level, color\n",
    "    \n",
    "#     def run_detection(self):\n",
    "#         \"\"\"Main detection loop using MediaPipe\"\"\"\n",
    "#         cap = cv2.VideoCapture(0)\n",
    "        \n",
    "#         # Set camera properties\n",
    "#         cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "#         cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "#         cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "        \n",
    "#         print(\"MediaPipe Drowsiness Detection System Started...\")\n",
    "#         print(\"Press 'q' to quit\")\n",
    "#         print(f\"EAR Threshold: {self.EAR_THRESHOLD}\")\n",
    "#         print(f\"MAR Threshold: {self.MAR_THRESHOLD}\")\n",
    "#         print(f\"Consecutive Frames: {self.CONSECUTIVE_FRAMES}\")\n",
    "#         print(\"Calibrating head pose... Please look straight ahead for a few seconds\")\n",
    "        \n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "            \n",
    "#             # Flip frame horizontally for mirror effect\n",
    "#             frame = cv2.flip(frame, 1)\n",
    "#             height, width, _ = frame.shape\n",
    "            \n",
    "#             # Convert BGR to RGB for MediaPipe\n",
    "#             rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#             rgb_frame.flags.writeable = False\n",
    "            \n",
    "#             # Process the frame with MediaPipe\n",
    "#             results = self.face_mesh.process(rgb_frame)\n",
    "            \n",
    "#             # Convert back to BGR\n",
    "#             rgb_frame.flags.writeable = True\n",
    "            \n",
    "#             if results.multi_face_landmarks:\n",
    "#                 for face_landmarks in results.multi_face_landmarks:\n",
    "#                     # Extract eye landmarks for EAR calculation\n",
    "#                     left_eye = self.get_landmarks(face_landmarks, self.LEFT_EYE_SIMPLE, width, height)\n",
    "#                     right_eye = self.get_landmarks(face_landmarks, self.RIGHT_EYE_SIMPLE, width, height)\n",
    "                    \n",
    "#                     # Extract mouth landmarks for MAR calculation\n",
    "#                     mouth = self.get_landmarks(face_landmarks, self.MOUTH_SIMPLE, width, height)\n",
    "                    \n",
    "#                     # Calculate ratios\n",
    "#                     left_ear = self.eye_aspect_ratio(left_eye)\n",
    "#                     right_ear = self.eye_aspect_ratio(right_eye)\n",
    "#                     ear = (left_ear + right_ear) / 2.0\n",
    "                    \n",
    "#                     mar = self.mouth_aspect_ratio(mouth)\n",
    "                    \n",
    "#                     # Get head pose\n",
    "#                     head_angles = self.get_head_pose_mediapipe(face_landmarks, width, height)\n",
    "                    \n",
    "#                     # FIXED: Calibrate head pose first\n",
    "#                     self.calibrate_head_pose(head_angles)\n",
    "                    \n",
    "#                     # Detect blinks and calculate frequency\n",
    "#                     self.detect_blink(ear)\n",
    "#                     blink_freq = self.get_blink_frequency()\n",
    "                    \n",
    "#                     # Analyze drowsiness\n",
    "#                     indicators, drowsiness_score = self.analyze_drowsiness(\n",
    "#                         ear, mar, head_angles, blink_freq\n",
    "#                     )\n",
    "                    \n",
    "#                     # Trigger alerts\n",
    "#                     alert_level, alert_color = self.trigger_alert(drowsiness_score, indicators)\n",
    "                    \n",
    "#                     # UPDATED: Draw eye and mouth landmarks WITHOUT contour lines - only points\n",
    "#                     left_eye_full = self.get_landmarks(face_landmarks, self.LEFT_EYE, width, height)\n",
    "#                     right_eye_full = self.get_landmarks(face_landmarks, self.RIGHT_EYE, width, height)\n",
    "#                     mouth_full = self.get_landmarks(face_landmarks, self.MOUTH_SIMPLE, width, height)\n",
    "                    \n",
    "#                     # REMOVED: All contour line drawing (drawContours calls)\n",
    "                    \n",
    "#                     # Draw individual landmark points with different colors and sizes\n",
    "#                     # Left eye points in cyan\n",
    "#                     for point in left_eye_full:\n",
    "#                         cv2.circle(frame, tuple(point), 2, (255, 255, 0), -1)  # Cyan, size 2\n",
    "                    \n",
    "#                     # Right eye points in cyan\n",
    "#                     for point in right_eye_full:\n",
    "#                         cv2.circle(frame, tuple(point), 2, (255, 255, 0), -1)  # Cyan, size 2\n",
    "                    \n",
    "#                     # Mouth points in yellow\n",
    "#                     for point in mouth_full:\n",
    "#                         cv2.circle(frame, tuple(point), 2, (0, 255, 255), -1)  # Yellow, size 2\n",
    "                    \n",
    "#                     # Draw face bounding rectangle\n",
    "#                     face_points = self.get_landmarks(face_landmarks, self.FACE_OVAL, width, height)\n",
    "#                     x, y, w, h = cv2.boundingRect(face_points)\n",
    "#                     cv2.rectangle(frame, (x, y), (x + w, y + h), alert_color, 2)\n",
    "                    \n",
    "#                     # Display metrics\n",
    "#                     y_offset = 30\n",
    "#                     cv2.putText(frame, f\"Alert Level: {alert_level}\", (10, y_offset), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, alert_color, 2)\n",
    "                    \n",
    "#                     y_offset += 30\n",
    "#                     cv2.putText(frame, f\"Drowsiness Score: {drowsiness_score}\", (10, y_offset), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                    \n",
    "#                     y_offset += 25\n",
    "#                     cv2.putText(frame, f\"EAR: {ear:.3f} (Thresh: {self.EAR_THRESHOLD})\", (10, y_offset), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "#                     y_offset += 20\n",
    "#                     cv2.putText(frame, f\"MAR: {mar:.3f} (Thresh: {self.MAR_THRESHOLD})\", (10, y_offset), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "#                     y_offset += 20\n",
    "#                     if self.pose_calibrated:\n",
    "#                         # Show deviation from reference pose\n",
    "#                         ref_pitch, ref_yaw, ref_roll = self.reference_pose\n",
    "#                         pitch_dev = abs(head_angles[0] - ref_pitch)\n",
    "#                         yaw_dev = abs(head_angles[1] - ref_yaw)\n",
    "#                         roll_dev = abs(head_angles[2] - ref_roll)\n",
    "#                         cv2.putText(frame, f\"Head Deviation: P:{pitch_dev:.1f} Y:{yaw_dev:.1f} R:{roll_dev:.1f}\", \n",
    "#                                    (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "#                     else:\n",
    "#                         cv2.putText(frame, f\"Calibrating... ({self.calibration_frames}/30)\", \n",
    "#                                    (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "                    \n",
    "#                     y_offset += 20\n",
    "#                     cv2.putText(frame, f\"Blinks/min: {blink_freq}\", (10, y_offset), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "#                     y_offset += 20\n",
    "#                     cv2.putText(frame, f\"Total Blinks: {self.total_blinks}\", (10, y_offset), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "#                     y_offset += 20\n",
    "#                     cv2.putText(frame, f\"EAR Counter: {self.ear_counter}/{self.CONSECUTIVE_FRAMES}\", (10, y_offset), \n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "#                     # Display active indicators\n",
    "#                     y_offset += 25\n",
    "#                     for indicator, active in indicators.items():\n",
    "#                         if active:\n",
    "#                             cv2.putText(frame, f\"{indicator.replace('_', ' ').title()}: ACTIVE\", \n",
    "#                                        (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "#                             y_offset += 20\n",
    "            \n",
    "#             # Show frame\n",
    "#             cv2.imshow('MediaPipe Drowsiness Detection System', frame)\n",
    "            \n",
    "#             # Break loop on 'q' key press\n",
    "#             if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "        \n",
    "#         # Cleanup\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "        \n",
    "#         # Stop any playing alerts\n",
    "#         if self.alert_playing and self.alert_channel:\n",
    "#             try:\n",
    "#                 self.alert_channel.stop()\n",
    "#             except:\n",
    "#                 pass\n",
    "                \n",
    "#         self.face_mesh.close()\n",
    "#         pygame.mixer.quit()\n",
    "class MediaPipeDrowsinessDetector:\n",
    "    def __init__(self):\n",
    "        # Initialize MediaPipe Face Mesh\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        \n",
    "        # Initialize Face Mesh model\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        \n",
    "        # ALL MediaPipe Face Mesh landmark indices - COMPLETE 468 POINTS\n",
    "        # Left eye region - ALL points around left eye area (indices 0-230 approximately)\n",
    "        self.LEFT_EYE = list(range(0, 17)) + list(range(18, 23)) + list(range(24, 35)) + \\\n",
    "                       list(range(36, 42)) + list(range(54, 68)) + list(range(69, 76)) + \\\n",
    "                       list(range(103, 134)) + list(range(139, 155)) + list(range(157, 173)) + \\\n",
    "                       list(range(189, 246)) + [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161]\n",
    "        \n",
    "        # Right eye region - ALL points around right eye area (indices 230-468 approximately)  \n",
    "        self.RIGHT_EYE = list(range(249, 280)) + list(range(283, 300)) + list(range(334, 365)) + \\\n",
    "                        list(range(373, 390)) + list(range(398, 415)) + list(range(435, 468)) + \\\n",
    "                        [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384]\n",
    "        \n",
    "        # Mouth region - ALL points around mouth, lips, and surrounding area\n",
    "        self.MOUTH_COMPREHENSIVE = list(range(0, 17)) + list(range(61, 96)) + list(range(146, 180)) + \\\n",
    "                                  list(range(267, 325)) + list(range(375, 415)) + \\\n",
    "                                  [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308, 415, 310, 311, 312]\n",
    "        \n",
    "        # ENHANCED: More precise eye landmarks for EAR calculation\n",
    "        # Left eye: outer corner, multiple top points, inner corner, multiple bottom points\n",
    "        self.LEFT_EYE_SIMPLE = [33, 160, 159, 158, 133, 153, 145, 144]  # 8 points for better precision\n",
    "        # Right eye: outer corner, multiple top points, inner corner, multiple bottom points  \n",
    "        self.RIGHT_EYE_SIMPLE = [362, 385, 386, 387, 263, 373, 374, 380]  # 8 points for better precision\n",
    "        \n",
    "        # COMPREHENSIVE: Maximum mouth landmarks for superior MAR calculation\n",
    "        self.MOUTH_SIMPLE = list(range(0, 17)) + list(range(61, 96)) + list(range(146, 180)) + \\\n",
    "                           list(range(267, 325)) + list(range(375, 415)) + \\\n",
    "                           [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308, 415, 310, 311, 312]\n",
    "        \n",
    "        # OPTION: Use ALL 468 facial landmarks for complete coverage\n",
    "        self.ALL_FACE_LANDMARKS = list(range(468))  # All 468 MediaPipe landmarks\n",
    "        \n",
    "        # Head pose estimation landmarks\n",
    "        self.FACE_OVAL = [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67]\n",
    "        \n",
    "        # ADJUSTED: More realistic thresholds\n",
    "        self.EAR_THRESHOLD = 0.21\n",
    "        self.MAR_THRESHOLD = 0.7   \n",
    "        self.HEAD_POSE_THRESHOLD = 45  # FIXED: Increased threshold to reduce false positives\n",
    "        self.CONSECUTIVE_FRAMES = 25   \n",
    "        \n",
    "        # Counters and tracking variables\n",
    "        self.ear_counter = 0\n",
    "        self.mar_counter = 0\n",
    "        self.head_pose_counter = 0\n",
    "        self.blink_counter = 0\n",
    "        self.total_blinks = 0\n",
    "        \n",
    "        # Temporal tracking\n",
    "        self.ear_history = deque(maxlen=30)\n",
    "        self.mar_history = deque(maxlen=30)\n",
    "        self.head_pose_history = deque(maxlen=30)\n",
    "        self.blink_history = deque(maxlen=300)  # 10 seconds at 30fps\n",
    "        \n",
    "        # FIXED: Head pose calibration - store initial pose as reference\n",
    "        self.reference_pose = None\n",
    "        self.pose_calibrated = False\n",
    "        self.calibration_frames = 0\n",
    "        self.calibration_poses = []\n",
    "        \n",
    "        # Alert system with continuous beeping\n",
    "        pygame.mixer.init()\n",
    "        self.alert_sound = None\n",
    "        self.load_alert_sound()\n",
    "        self.alert_playing = False\n",
    "        self.alert_channel = None\n",
    "        \n",
    "        # Previous eye state for blink detection\n",
    "        self.prev_eye_closed = False\n",
    "        \n",
    "        # Time tracking\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def load_alert_sound(self):\n",
    "        \"\"\"Load alert sound with continuous beeping capability\"\"\"\n",
    "        try:\n",
    "            # Try to load a custom alert sound\n",
    "            self.alert_sound = pygame.mixer.Sound('alert.wav')\n",
    "        except:\n",
    "            # Create a continuous beep sound programmatically\n",
    "            sample_rate = 22050\n",
    "            duration = 0.5  # Shorter beep for continuous effect\n",
    "            frequency = 1000  # Higher frequency for urgency\n",
    "            t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "            \n",
    "            # Create a more urgent beep pattern\n",
    "            wave = np.sin(frequency * 2 * np.pi * t) * 0.4\n",
    "            # Add fade in/out to avoid clicking\n",
    "            fade_samples = int(0.05 * sample_rate)  # 50ms fade\n",
    "            wave[:fade_samples] *= np.linspace(0, 1, fade_samples)\n",
    "            wave[-fade_samples:] *= np.linspace(1, 0, fade_samples)\n",
    "            \n",
    "            sound_array = (wave * 32767).astype(np.int16)\n",
    "            sound_array = np.repeat(sound_array.reshape(len(sound_array), 1), 2, axis=1)\n",
    "            self.alert_sound = pygame.sndarray.make_sound(sound_array)\n",
    "    \n",
    "    def get_landmarks(self, landmarks, indices, width, height):\n",
    "        \"\"\"Extract specific landmarks and convert to pixel coordinates\"\"\"\n",
    "        points = []\n",
    "        for idx in indices:\n",
    "            x = int(landmarks.landmark[idx].x * width)\n",
    "            y = int(landmarks.landmark[idx].y * height)\n",
    "            points.append([x, y])\n",
    "        return np.array(points)\n",
    "    \n",
    "    def eye_aspect_ratio(self, eye_landmarks):\n",
    "        \"\"\"Enhanced Eye Aspect Ratio (EAR) calculation using multiple points\"\"\"\n",
    "        try:\n",
    "            # Multiple vertical distances for better accuracy\n",
    "            A1 = dist.euclidean(eye_landmarks[1], eye_landmarks[7])  # Top to bottom (vertical 1)\n",
    "            A2 = dist.euclidean(eye_landmarks[2], eye_landmarks[6])  # Top to bottom (vertical 2)\n",
    "            A3 = dist.euclidean(eye_landmarks[1], eye_landmarks[6])  # Cross vertical\n",
    "            A4 = dist.euclidean(eye_landmarks[2], eye_landmarks[7])  # Cross vertical\n",
    "            \n",
    "            # Horizontal distance (width of the eye)\n",
    "            C = dist.euclidean(eye_landmarks[0], eye_landmarks[4])  # Outer corner to inner corner\n",
    "            \n",
    "            # Enhanced EAR formula using multiple measurements\n",
    "            if C == 0:  # Prevent division by zero\n",
    "                return 0.3\n",
    "            \n",
    "            ear = (A1 + A2 + A3 + A4) / (4.0 * C)\n",
    "            return ear\n",
    "        except:\n",
    "            return 0.3\n",
    "    \n",
    "    def mouth_aspect_ratio(self, mouth_landmarks):\n",
    "        \"\"\"Enhanced Mouth Aspect Ratio (MAR) calculation using multiple points\"\"\"\n",
    "        try:\n",
    "            # Multiple vertical mouth measurements for better accuracy\n",
    "            A1 = dist.euclidean(mouth_landmarks[1], mouth_landmarks[7])   # Top to bottom (center)\n",
    "            A2 = dist.euclidean(mouth_landmarks[2], mouth_landmarks[6])   # Top to bottom (left-center)\n",
    "            A3 = dist.euclidean(mouth_landmarks[3], mouth_landmarks[5])   # Top to bottom (right-center)\n",
    "            A4 = dist.euclidean(mouth_landmarks[13], mouth_landmarks[19]) # Inner vertical distance\n",
    "            A5 = dist.euclidean(mouth_landmarks[14], mouth_landmarks[18]) # Inner vertical distance\n",
    "            \n",
    "            # Horizontal mouth distance (width)\n",
    "            C1 = dist.euclidean(mouth_landmarks[0], mouth_landmarks[4])   # Left corner to right corner\n",
    "            C2 = dist.euclidean(mouth_landmarks[12], mouth_landmarks[16]) # Inner horizontal distance\n",
    "            \n",
    "            if C1 == 0 or C2 == 0:  # Prevent division by zero\n",
    "                return 0.3\n",
    "                \n",
    "            # Enhanced MAR formula using multiple measurements\n",
    "            mar = (A1 + A2 + A3 + A4 + A5) / (3.0 * (C1 + C2))\n",
    "            return mar\n",
    "        except:\n",
    "            return 0.3\n",
    "    \n",
    "    def get_head_pose_mediapipe(self, landmarks, width, height):\n",
    "        \"\"\"Estimate head pose using MediaPipe 3D landmarks\"\"\"\n",
    "        # Key 3D model points for head pose estimation\n",
    "        model_points = np.array([\n",
    "            (0.0, 0.0, 0.0),             # Nose tip\n",
    "            (0.0, -330.0, -65.0),        # Chin\n",
    "            (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "            (225.0, 170.0, -135.0),      # Right eye right corner  \n",
    "            (-150.0, -150.0, -125.0),    # Left mouth corner\n",
    "            (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "        ])\n",
    "        \n",
    "        # Corresponding 2D image points from MediaPipe landmarks\n",
    "        image_points = np.array([\n",
    "            [landmarks.landmark[1].x * width, landmarks.landmark[1].y * height],    # Nose tip\n",
    "            [landmarks.landmark[152].x * width, landmarks.landmark[152].y * height], # Chin\n",
    "            [landmarks.landmark[33].x * width, landmarks.landmark[33].y * height],   # Left eye corner\n",
    "            [landmarks.landmark[263].x * width, landmarks.landmark[263].y * height], # Right eye corner\n",
    "            [landmarks.landmark[61].x * width, landmarks.landmark[61].y * height],   # Left mouth corner\n",
    "            [landmarks.landmark[291].x * width, landmarks.landmark[291].y * height]  # Right mouth corner\n",
    "        ], dtype=\"double\")\n",
    "        \n",
    "        # Camera internals\n",
    "        focal_length = width\n",
    "        center = (width/2, height/2)\n",
    "        camera_matrix = np.array([\n",
    "            [focal_length, 0, center[0]],\n",
    "            [0, focal_length, center[1]],\n",
    "            [0, 0, 1]\n",
    "        ], dtype=\"double\")\n",
    "        \n",
    "        dist_coeffs = np.zeros((4,1))\n",
    "        \n",
    "        # Solve PnP\n",
    "        try:\n",
    "            success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "                model_points, image_points, camera_matrix, dist_coeffs, \n",
    "                flags=cv2.SOLVEPNP_ITERATIVE\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                # Convert rotation vector to angles\n",
    "                rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "                angles, _, _, _, _, _ = cv2.RQDecomp3x3(rotation_matrix)\n",
    "                return angles[0], angles[1], angles[2]  # pitch, yaw, roll\n",
    "            else:\n",
    "                return 0, 0, 0\n",
    "        except:\n",
    "            return 0, 0, 0\n",
    "    \n",
    "    def calibrate_head_pose(self, head_angles):\n",
    "        \"\"\"FIXED: Calibrate reference head pose for better detection\"\"\"\n",
    "        if not self.pose_calibrated:\n",
    "            self.calibration_poses.append(head_angles)\n",
    "            self.calibration_frames += 1\n",
    "            \n",
    "            if self.calibration_frames >= 30:  # Calibrate over 30 frames\n",
    "                # Calculate average pose as reference\n",
    "                avg_pitch = np.mean([pose[0] for pose in self.calibration_poses])\n",
    "                avg_yaw = np.mean([pose[1] for pose in self.calibration_poses])\n",
    "                avg_roll = np.mean([pose[2] for pose in self.calibration_poses])\n",
    "                \n",
    "                self.reference_pose = (avg_pitch, avg_yaw, avg_roll)\n",
    "                self.pose_calibrated = True\n",
    "                print(f\"Head pose calibrated: P:{avg_pitch:.1f} Y:{avg_yaw:.1f} R:{avg_roll:.1f}\")\n",
    "                \n",
    "        return self.pose_calibrated\n",
    "    \n",
    "    def detect_blink(self, ear):\n",
    "        \"\"\"Detect blinks based on EAR\"\"\"\n",
    "        eye_closed = ear < self.EAR_THRESHOLD\n",
    "        \n",
    "        # Detect transition from closed to open (blink completion)\n",
    "        if self.prev_eye_closed and not eye_closed:\n",
    "            self.total_blinks += 1\n",
    "            self.blink_history.append(time.time())\n",
    "        \n",
    "        self.prev_eye_closed = eye_closed\n",
    "        return eye_closed\n",
    "    \n",
    "    def get_blink_frequency(self):\n",
    "        \"\"\"Calculate blinks per minute\"\"\"\n",
    "        current_time = time.time()\n",
    "        # Remove blinks older than 60 seconds\n",
    "        self.blink_history = deque([t for t in self.blink_history if current_time - t <= 60], \n",
    "                                  maxlen=300)\n",
    "        return len(self.blink_history)\n",
    "    \n",
    "    def analyze_drowsiness(self, ear, mar, head_angles, blink_freq):\n",
    "        \"\"\"Multi-factor drowsiness analysis\"\"\"\n",
    "        drowsiness_indicators = {\n",
    "            'eye_closure': False,\n",
    "            'yawning': False,\n",
    "            'head_nodding': False,\n",
    "            'low_blink_rate': False\n",
    "        }\n",
    "        \n",
    "        drowsiness_score = 0\n",
    "        \n",
    "        # FIXED: Eye closure detection with better logic\n",
    "        if ear < self.EAR_THRESHOLD:\n",
    "            self.ear_counter += 1\n",
    "            if self.ear_counter >= self.CONSECUTIVE_FRAMES:\n",
    "                drowsiness_indicators['eye_closure'] = True\n",
    "                drowsiness_score += 40\n",
    "        else:\n",
    "            self.ear_counter = 0\n",
    "        \n",
    "        # FIXED: Yawn detection with better threshold\n",
    "        if mar > self.MAR_THRESHOLD:\n",
    "            self.mar_counter += 1\n",
    "            if self.mar_counter >= 15:  # Increased threshold for yawning\n",
    "                drowsiness_indicators['yawning'] = True\n",
    "                drowsiness_score += 25\n",
    "        else:\n",
    "            self.mar_counter = 0\n",
    "        \n",
    "        # FIXED: Head pose analysis with calibration\n",
    "        if self.pose_calibrated:\n",
    "            pitch, yaw, roll = head_angles\n",
    "            ref_pitch, ref_yaw, ref_roll = self.reference_pose\n",
    "            \n",
    "            # Calculate deviations from reference pose\n",
    "            pitch_deviation = abs(pitch - ref_pitch)\n",
    "            yaw_deviation = abs(yaw - ref_yaw)\n",
    "            roll_deviation = abs(roll - ref_roll)\n",
    "            \n",
    "            # Only trigger if significant deviation from normal pose\n",
    "            if (pitch_deviation > self.HEAD_POSE_THRESHOLD or \n",
    "                yaw_deviation > self.HEAD_POSE_THRESHOLD or \n",
    "                roll_deviation > self.HEAD_POSE_THRESHOLD):\n",
    "                self.head_pose_counter += 1\n",
    "                if self.head_pose_counter >= 30:  # Increased threshold\n",
    "                    drowsiness_indicators['head_nodding'] = True\n",
    "                    drowsiness_score += 20\n",
    "            else:\n",
    "                self.head_pose_counter = 0\n",
    "        \n",
    "        # Low blink rate detection (normal: 15-20 blinks/minute)\n",
    "        if blink_freq < 6:  # Very low blink rate\n",
    "            drowsiness_indicators['low_blink_rate'] = True\n",
    "            drowsiness_score += 15\n",
    "        \n",
    "        # Store history for temporal analysis\n",
    "        self.ear_history.append(ear)\n",
    "        self.mar_history.append(mar)\n",
    "        self.head_pose_history.append(head_angles)\n",
    "        \n",
    "        return drowsiness_indicators, drowsiness_score\n",
    "    \n",
    "    def trigger_alert(self, drowsiness_score, indicators):\n",
    "        \"\"\"Trigger appropriate alerts with continuous beeping for critical states\"\"\"\n",
    "        if drowsiness_score >= 60:  # High drowsiness\n",
    "            alert_level = \"CRITICAL\"\n",
    "            color = (0, 0, 255)  # Red\n",
    "            should_beep = True\n",
    "        elif drowsiness_score >= 35:  # Moderate drowsiness\n",
    "            alert_level = \"WARNING\"\n",
    "            color = (0, 165, 255)  # Orange\n",
    "            should_beep = True\n",
    "        elif drowsiness_score >= 20:  # Mild drowsiness\n",
    "            alert_level = \"CAUTION\"\n",
    "            color = (0, 255, 255)  # Yellow\n",
    "            should_beep = False\n",
    "        else:\n",
    "            alert_level = \"NORMAL\"\n",
    "            color = (0, 255, 0)  # Green\n",
    "            should_beep = False\n",
    "        \n",
    "        # CONTINUOUS BEEPING LOGIC\n",
    "        if should_beep and self.alert_sound:\n",
    "            if not self.alert_playing:\n",
    "                # Start continuous beeping\n",
    "                try:\n",
    "                    self.alert_channel = self.alert_sound.play(-1)  # -1 means loop indefinitely\n",
    "                    self.alert_playing = True\n",
    "                    print(f\"ðŸš¨ ALERT STARTED: {alert_level}\")\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            # Stop beeping if alert level is low\n",
    "            if self.alert_playing:\n",
    "                try:\n",
    "                    if self.alert_channel:\n",
    "                        self.alert_channel.stop()\n",
    "                    self.alert_playing = False\n",
    "                    print(f\"âœ… ALERT STOPPED: {alert_level}\")\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        return alert_level, color\n",
    "    \n",
    "    def run_detection(self):\n",
    "        \"\"\"Main detection loop using MediaPipe\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        # Set camera properties\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "        \n",
    "        print(\"MediaPipe Drowsiness Detection System Started...\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "        print(f\"EAR Threshold: {self.EAR_THRESHOLD}\")\n",
    "        print(f\"MAR Threshold: {self.MAR_THRESHOLD}\")\n",
    "        print(f\"Consecutive Frames: {self.CONSECUTIVE_FRAMES}\")\n",
    "        print(\"Calibrating head pose... Please look straight ahead for a few seconds\")\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Flip frame horizontally for mirror effect\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            height, width, _ = frame.shape\n",
    "            \n",
    "            # Convert BGR to RGB for MediaPipe\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            rgb_frame.flags.writeable = False\n",
    "            \n",
    "            # Process the frame with MediaPipe\n",
    "            results = self.face_mesh.process(rgb_frame)\n",
    "            \n",
    "            # Convert back to BGR\n",
    "            rgb_frame.flags.writeable = True\n",
    "            \n",
    "            if results.multi_face_landmarks:\n",
    "                for face_landmarks in results.multi_face_landmarks:\n",
    "                    # Extract eye landmarks for EAR calculation\n",
    "                    left_eye = self.get_landmarks(face_landmarks, self.LEFT_EYE_SIMPLE, width, height)\n",
    "                    right_eye = self.get_landmarks(face_landmarks, self.RIGHT_EYE_SIMPLE, width, height)\n",
    "                    \n",
    "                    # Extract mouth landmarks for MAR calculation\n",
    "                    mouth = self.get_landmarks(face_landmarks, self.MOUTH_SIMPLE, width, height)\n",
    "                    \n",
    "                    # Calculate ratios\n",
    "                    left_ear = self.eye_aspect_ratio(left_eye)\n",
    "                    right_ear = self.eye_aspect_ratio(right_eye)\n",
    "                    ear = (left_ear + right_ear) / 2.0\n",
    "                    \n",
    "                    mar = self.mouth_aspect_ratio(mouth)\n",
    "                    \n",
    "                    # Get head pose\n",
    "                    head_angles = self.get_head_pose_mediapipe(face_landmarks, width, height)\n",
    "                    \n",
    "                    # FIXED: Calibrate head pose first\n",
    "                    self.calibrate_head_pose(head_angles)\n",
    "                    \n",
    "                    # Detect blinks and calculate frequency\n",
    "                    self.detect_blink(ear)\n",
    "                    blink_freq = self.get_blink_frequency()\n",
    "                    \n",
    "                    # Analyze drowsiness\n",
    "                    indicators, drowsiness_score = self.analyze_drowsiness(\n",
    "                        ear, mar, head_angles, blink_freq\n",
    "                    )\n",
    "                    \n",
    "                    # Trigger alerts\n",
    "                    alert_level, alert_color = self.trigger_alert(drowsiness_score, indicators)\n",
    "                    \n",
    "                    # OPTION 1: Use specific eye/mouth regions (current approach)\n",
    "                    left_eye_full = self.get_landmarks(face_landmarks, self.LEFT_EYE, width, height)\n",
    "                    right_eye_full = self.get_landmarks(face_landmarks, self.RIGHT_EYE, width, height)\n",
    "                    mouth_full = self.get_landmarks(face_landmarks, self.MOUTH_SIMPLE, width, height)\n",
    "                    \n",
    "                    # OPTION 2: Use ALL 468 landmarks (uncomment below to use all points)\n",
    "                    # all_landmarks = self.get_landmarks(face_landmarks, self.ALL_FACE_LANDMARKS, width, height)\n",
    "                    \n",
    "                    # Draw individual landmark points with different colors and sizes\n",
    "                    # Left eye points in cyan\n",
    "                    for point in left_eye_full:\n",
    "                        cv2.circle(frame, tuple(point), 2, (255, 255, 0), -1)  # Cyan, size 2\n",
    "                    \n",
    "                    # Right eye points in cyan\n",
    "                    for point in right_eye_full:\n",
    "                        cv2.circle(frame, tuple(point), 2, (255, 255, 0), -1)  # Cyan, size 2\n",
    "                    \n",
    "                    # Mouth points in yellow\n",
    "                    for point in mouth_full:\n",
    "                        cv2.circle(frame, tuple(point), 2, (0, 255, 255), -1)  # Yellow, size 2\n",
    "                    \n",
    "                    # OPTION 2: Draw ALL 468 landmarks (uncomment to use)\n",
    "                    # for i, point in enumerate(all_landmarks):\n",
    "                    #     if i < 200:  # First 200 points in red\n",
    "                    #         cv2.circle(frame, tuple(point), 1, (0, 0, 255), -1)\n",
    "                    #     elif i < 400:  # Next 200 points in green\n",
    "                    #         cv2.circle(frame, tuple(point), 1, (0, 255, 0), -1)\n",
    "                    #     else:  # Remaining points in blue\n",
    "                    #         cv2.circle(frame, tuple(point), 1, (255, 0, 0), -1)\n",
    "                    \n",
    "                    # Draw face bounding rectangle\n",
    "                    face_points = self.get_landmarks(face_landmarks, self.FACE_OVAL, width, height)\n",
    "                    x, y, w, h = cv2.boundingRect(face_points)\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), alert_color, 2)\n",
    "                    \n",
    "                    # Display metrics\n",
    "                    y_offset = 30\n",
    "                    cv2.putText(frame, f\"Alert Level: {alert_level}\", (10, y_offset), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, alert_color, 2)\n",
    "                    \n",
    "                    y_offset += 30\n",
    "                    cv2.putText(frame, f\"Drowsiness Score: {drowsiness_score}\", (10, y_offset), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                    \n",
    "                    y_offset += 25\n",
    "                    cv2.putText(frame, f\"EAR: {ear:.3f} (Thresh: {self.EAR_THRESHOLD})\", (10, y_offset), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "                    y_offset += 20\n",
    "                    cv2.putText(frame, f\"MAR: {mar:.3f} (Thresh: {self.MAR_THRESHOLD})\", (10, y_offset), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "                    y_offset += 20\n",
    "                    if self.pose_calibrated:\n",
    "                        # Show deviation from reference pose\n",
    "                        ref_pitch, ref_yaw, ref_roll = self.reference_pose\n",
    "                        pitch_dev = abs(head_angles[0] - ref_pitch)\n",
    "                        yaw_dev = abs(head_angles[1] - ref_yaw)\n",
    "                        roll_dev = abs(head_angles[2] - ref_roll)\n",
    "                        cv2.putText(frame, f\"Head Deviation: P:{pitch_dev:.1f} Y:{yaw_dev:.1f} R:{roll_dev:.1f}\", \n",
    "                                   (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    else:\n",
    "                        cv2.putText(frame, f\"Calibrating... ({self.calibration_frames}/30)\", \n",
    "                                   (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "                    \n",
    "                    y_offset += 20\n",
    "                    cv2.putText(frame, f\"Blinks/min: {blink_freq}\", (10, y_offset), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "                    y_offset += 20\n",
    "                    cv2.putText(frame, f\"Total Blinks: {self.total_blinks}\", (10, y_offset), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "                    y_offset += 20\n",
    "                    cv2.putText(frame, f\"EAR Counter: {self.ear_counter}/{self.CONSECUTIVE_FRAMES}\", (10, y_offset), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                    \n",
    "                    # Display active indicators\n",
    "                    y_offset += 25\n",
    "                    for indicator, active in indicators.items():\n",
    "                        if active:\n",
    "                            cv2.putText(frame, f\"{indicator.replace('_', ' ').title()}: ACTIVE\", \n",
    "                                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "                            y_offset += 20\n",
    "            \n",
    "            # Show frame\n",
    "            cv2.imshow('MediaPipe Drowsiness Detection System', frame)\n",
    "            \n",
    "            # Break loop on 'q' key press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Stop any playing alerts\n",
    "        if self.alert_playing and self.alert_channel:\n",
    "            try:\n",
    "                self.alert_channel.stop()\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        self.face_mesh.close()\n",
    "        pygame.mixer.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a175405-51a8-4932-baf8-df60372514d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediaPipe Drowsiness Detection System Started...\n",
      "Press 'q' to quit\n",
      "EAR Threshold: 0.21\n",
      "MAR Threshold: 0.7\n",
      "Consecutive Frames: 25\n",
      "Calibrating head pose... Please look straight ahead for a few seconds\n",
      "ðŸš¨ ALERT STARTED: WARNING\n",
      "Head pose calibrated: P:-176.4 Y:2.6 R:-1.8\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: WARNING\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: CRITICAL\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: CRITICAL\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: CRITICAL\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: CRITICAL\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: CRITICAL\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: CRITICAL\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: WARNING\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: WARNING\n",
      "âœ… ALERT STOPPED: CAUTION\n",
      "ðŸš¨ ALERT STARTED: WARNING\n",
      "âœ… ALERT STOPPED: CAUTION\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    detector = MediaPipeDrowsinessDetector()\n",
    "    detector.run_detection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
